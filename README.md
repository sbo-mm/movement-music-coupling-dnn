# TMP README

## Towards a Deep Learning Approach to Transform Motion to Sound using Motiongrams and Spectrogram Inversion

This repository presents the initial results from a developed deep neural network that learns a mapping from a motiongram to a spectrogram. We show that this system can be used in creative music production as well as in analytical tasks such as dance- or music genre classification [<i>Excerpt from the current abstract</i>].

### TODO

(<i>@all: Feel free to update and/or add items to the TODO list</i>)

- [ ] Elaborate README File #desc @sophus 
	- [ ] Add manual
	- [ ] Add correct license
- [ ] Share all datasets online and implement python function to retrieve #code @sophus
	- [x] <i>Update dataset to reflect computation changes</i>

### In Progress
- [ ] Fix a bug (incorrect scaling) in the VAE loss #bug @sophus
- [ ] Perform new experiments with the applied updates #futurework @all 

### Done âœ“
- [x] Migrate the current implementation to work with custom python package #code @sophus
- [x] Update the motiongram computation and inlude source in repo #code @sophus
- [x] Update spectrogram computation (avoid MEL-transform and dB-scaling) #code @sophus/stefano
