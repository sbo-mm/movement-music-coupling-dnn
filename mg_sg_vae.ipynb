{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e486b3b3",
   "metadata": {},
   "source": [
    "#### Meaningful description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Import and setup the tensorflow session\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sanity check to ensure we are on correct tensorflow version\n",
    "print(\"Using Tensorflow v. %s\"%tf.__version__); assert tf.__version__ == \"2.4.1\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tf contrib modules\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Import Keras modules\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Import from custom modules\n",
    "from data_utils.mg_sg_generator import strip_file\n",
    "from data_utils.mg_sg_generator import get_dataset_for, splice_spectrogram_patches\n",
    "from data_utils.mg_sg_generator import prepare_dataset_for_training, prepare_dataset_for_evaluation\n",
    "\n",
    "from data_utils.sg_preprocessor import spectrogram2audio\n",
    "from data_utils.sg_preprocessor import inverse_normalize_db_0_1, FNCOLS \n",
    "\n",
    "# Import custom tensorflow utilities\n",
    "from tf_extensions import KERAS_BACKEND as K\n",
    "from tf_extensions.tf_custom.models import GaussianBetaVAE\n",
    "from tf_extensions.tf_custom.models import make_cnn_vae_encoder, make_dense_vae_decoder\n",
    "\n",
    "# Common utilities\n",
    "import numpy as np\n",
    "\n",
    "# Display utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Pathing (using absolute paths for now)\n",
    "BASE_DATA_PATH = \"/home/sbol13/sbol_data\"\n",
    "MW_SAVE_PATH   = BASE_DATA_PATH + \"/model_weights\"\n",
    "\n",
    "print(K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9242e9",
   "metadata": {},
   "source": [
    "#### Utility functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f60615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mvn(x_mu, x_logvar):\n",
    "    x_sig = np.exp(x_logvar)\n",
    "    batch = x_sig.shape[0]\n",
    "    mvn = tfp.distributions.MultivariateNormalDiag(\n",
    "        loc=x_mu, scale_diag=x_sig\n",
    "    )\n",
    "    return mvn.sample(shape=[batch]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7342f8",
   "metadata": {},
   "source": [
    "### Prepare and Setup Data\n",
    "Construct and/or load the datasets we need for the various experiments:\n",
    "- Linear Spectrogram <b>(r128xc128)</b>, latent dim: 256\n",
    "- Linear Spectrogram <b>(r512xc128)</b>, latent dim: 256\n",
    "- Linear Spectrogram <b>(r512xc128)</b>, latent dim: 256x4 (2048)\n",
    "\n",
    "Where rNUM indicated the FFT bins (i.e. the FFT-size) used to compute the spectrograms we are attempting to predict. cNUM is simply the STFT columns, which are a direct result of the <i>hop_size</i> we use.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8edb6",
   "metadata": {},
   "source": [
    "#### Globals for all experiments: \n",
    "Some variables are held constant throughout the different models under experimentation.\n",
    "We use the block below to define these, such that we can re-use them throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18215108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience lambda(s) to retrieve information\n",
    "# about a relevant .tfrecords file (nexamples, filepath)\n",
    "TFR_TRAIN      = lambda tfr_dict: tfr_dict[\"train\"]\n",
    "TFR_VALIDATION = lambda tfr_dict: tfr_dict[\"validation\"]\n",
    "TFR_TEST       = lambda tfr_dict: tfr_dict[\"test\"]\n",
    "TFR_PATH       = lambda tfr_tuple: tfr_tuple[-1]\n",
    "\n",
    "# Define a small lambda to compute the columns\n",
    "# to use for each nn input. TODO: FINISH\n",
    "NCOLS = lambda hl: FNCOLS(hl)\n",
    "\n",
    "# Define a small lambda to compute the rows\n",
    "# to use for each nn input.\n",
    "NROWS = lambda nfft: nfft//2 + 1\n",
    "\n",
    "# Define the train-size percentage.\n",
    "# This value indicates how much of the dataset is reserved for training. \n",
    "# The remaining is split evenly into validation/testing.\n",
    "# (i.e. a split could be 70-15-15)\n",
    "TRAIN_SIZE = 0.7\n",
    "\n",
    "# Define a flag to indicate whether we should\n",
    "# recompute all the datasets.\n",
    "DS_OVERWRITE = False\n",
    "\n",
    "# Define the beta-value to use for tuning the VAE(s).\n",
    "VAE_BETA = 1.\n",
    "\n",
    "# Define the learning rate to apply across\n",
    "# all training conditions.\n",
    "BASE_LR = 2e-04 \n",
    "\n",
    "# Define the optimizer to use for training across\n",
    "# all conditions.\n",
    "OPTIMIZER = optimizers.Adam(lr=BASE_LR)\n",
    "\n",
    "# Define batch size to use for training.\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Define a lambda to retrieve the step size(s).\n",
    "STEPS_PER_EPOCH = lambda tfr_tuple: tfr_tuple[0] // BATCH_SIZE\n",
    "\n",
    "# Lambda to retrieve savepath to model weights\n",
    "MW_SAVEPATH_FN = lambda meta_dict:\\\n",
    "    f'{MW_SAVE_PATH}/{strip_file(TFR_PATH(TFR_TRAIN(meta_dict)))}.h5'\n",
    "\n",
    "# Define a function to return a callback for model saving. \n",
    "# (setup saving weights incrementally)\n",
    "def CHECKPOINT_CALLBACK(meta_dict, save_step=20):\n",
    "    # Compute save frequency\n",
    "    SF = STEPS_PER_EPOCH(TFR_TRAIN(meta_dict)) * save_step \n",
    "\n",
    "    # Make callback instance\n",
    "    CHECKPOINT = ModelCheckpoint(\n",
    "        MW_SAVEPATH_FN(meta_dict), \n",
    "        monitor           = 'ELBO', \n",
    "        save_freq         = SF,\n",
    "        save_best_only    = True,\n",
    "        save_weights_only = True,\n",
    "        mode              ='auto',\n",
    "        verbose           = 1\n",
    "    )\n",
    "    return CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81afca",
   "metadata": {},
   "source": [
    "#### Approach 1 (AP1): Linspect r128xc128, latent dim 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfe3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FFT-size.\n",
    "ap1_nfft = 256\n",
    "\n",
    "# Define the hop_length (overlap)\n",
    "# in samples.\n",
    "ap1_overlap = ap1_nfft // 2\n",
    "\n",
    "# Define the latent dimension size.\n",
    "ap1_latent_dim = 256\n",
    "\n",
    "# Fetch a dataset that fullfils the \n",
    "# requirements for this condition.\n",
    "ap1_fout = get_dataset_for(\n",
    "    nfft               = ap1_nfft,\n",
    "    overlap            = ap1_overlap,\n",
    "    train_size         = TRAIN_SIZE, \n",
    "    overwrite_existing = DS_OVERWRITE\n",
    ")\n",
    "\n",
    "# Prepare the input/output shape(s).\n",
    "ap1_inout_shape_train = (NROWS(ap1_nfft), TFR_TRAIN(ap1_fout)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534afc70",
   "metadata": {},
   "source": [
    "##### Make and compile and instance of the deep learning model (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a deep learning model (VAE) \n",
    "# for this condition.\n",
    "ap1_vae = GaussianBetaVAE(\n",
    "    N                   = TFR_TRAIN(ap1_fout)[0],\n",
    "    M                   = BATCH_SIZE,\n",
    "    beta                = VAE_BETA,\n",
    "    input_dim           = ap1_inout_shape_train,\n",
    "    latent_dim          = ap1_latent_dim,\n",
    "    create_encoder_func = make_cnn_vae_encoder,\n",
    "    create_decoder_func = make_dense_vae_decoder\n",
    ")\n",
    "\n",
    "# Compile the model and make ready for training.\n",
    "ap1_vae.custom_compile(optimizer=OPTIMIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20753c9f",
   "metadata": {},
   "source": [
    "##### Train the model (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset(s) for training.\n",
    "ap1_ds_train = prepare_dataset_for_training(\n",
    "    filename     = TFR_PATH(TFR_TRAIN(ap1_fout)),\n",
    "    batch_size   = BATCH_SIZE,\n",
    "    cast_to_type = K.floatx()\n",
    ")\n",
    "\n",
    "ap1_ds_validation = prepare_dataset_for_training(\n",
    "    filename     = TFR_PATH(TFR_VALIDATION(ap1_fout)),\n",
    "    batch_size   = BATCH_SIZE,\n",
    "    cast_to_type = K.floatx()\n",
    ")\n",
    "\n",
    "# Train the vae model.\n",
    "ap1_history = ap1_vae.fit(\n",
    "    ap1_ds_train,\n",
    "    steps_per_epoch  = STEPS_PER_EPOCH(TFR_TRAIN(ap1_fout)),\n",
    "    validation_data  = ap1_ds_validation,\n",
    "    validation_steps = STEPS_PER_EPOCH(TFR_VALIDATION(ap1_fout)),\n",
    "    epochs           = 500,\n",
    "    callbacks        = [CHECKPOINT_CALLBACK(ap1_fout)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267624b",
   "metadata": {},
   "source": [
    "#### Evaluate AP1 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta = TFR_TRAIN(ap1_fout)\n",
    "batch_div = train_meta[0] // train_meta[1]\n",
    "ap1_ds_test = prepare_dataset_for_evaluation(\n",
    "    filename     = TFR_PATH(TFR_TRAIN(ap1_fout)),\n",
    "    batch_size   = batch_div,\n",
    "    cast_to_type = tf.float64\n",
    ")\n",
    "ds_iter = iter(ap1_ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_, _ = ds_iter.get_next()\n",
    "mu_x, logvar_x, _, _ = ap1_vae.predict(mg_)\n",
    "print(mu_x.shape, logvar_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 1.\n",
    "specgram = np.empty(mu_x.shape)\n",
    "for p in range(batch_div):\n",
    "    mus_vector = mu_x[p, :]\n",
    "    sigs = np.exp(logvar_x[p, :])\n",
    "    cov_matrix = np.identity(sigs.shape[0]) * sigs\n",
    "    #cov_matrix = np.matmul(scale_matrix, scale_matrix.T)\n",
    "    cov_matrix = cov_matrix * temp\n",
    "    sample = np.random.multivariate_normal(mus_vector, cov_matrix, 1)\n",
    "    specgram[p, ...] = sample\n",
    "    \n",
    "print(specgram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4555a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "specgram_patches_np = np.reshape(specgram, newshape=(batch_div, NROWS(ap1_nfft), -1))\n",
    "specgram_patches = [specgram_patches_np[p, ...] for p in range(batch_div)]\n",
    "specgram_spliced = splice_spectrogram_patches(specgram_patches, NCOLS(ap1_overlap))\n",
    "print(specgram_spliced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529de5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "axs.set_xticks([]); axs.set_yticks([]);\n",
    "axs.imshow(\n",
    "    np.flipud(specgram_spliced), \n",
    "    aspect=\"auto\", \n",
    "    cmap=\"Spectral_r\", \n",
    "    interpolation=\"bicubic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = spectrogram2audio(\n",
    "    specgram_spliced, \n",
    "    ap1_overlap, \n",
    "    True\n",
    ")\n",
    "print(y.shape)\n",
    "ipd.display(ipd.Audio(y, rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81959a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
