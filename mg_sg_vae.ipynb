{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e486b3b3",
   "metadata": {},
   "source": [
    "#### Meaningful description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Import and setup the tensorflow session\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sanity check to ensure we are on correct tensorflow version\n",
    "print(\"Using Tensorflow v. %s\"%tf.__version__); assert tf.__version__ == \"2.4.1\";\n",
    "\n",
    "# Setup a strategy to distribute resources and training\n",
    "# across all available GPUs on the system.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Check number of GPUs used by the distribution scope.\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras modules\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Import from custom modules\n",
    "from data_utils.mg_sg_generator import TFE_FN_TEMPLATE\n",
    "from data_utils.mg_sg_generator import prepare_dataset_for_training\n",
    "from data_utils.mg_sg_generator import get_dataset_for, extract_dlen_from_tfr\n",
    "\n",
    "from data_utils.sg_preprocessor import spectrogram2audio\n",
    "from data_utils.sg_preprocessor import inverse_normalize_db_0_1, FNCOLS \n",
    "\n",
    "# Import custom tensorflow utilities\n",
    "from tf_extensions.tf_custom.models import GaussianBetaVAE\n",
    "from tf_extensions.tf_custom.models import make_cnn_vae_encoder, make_dense_vae_decoder\n",
    "\n",
    "# Common utilities\n",
    "import numpy as np\n",
    "\n",
    "# Display utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7342f8",
   "metadata": {},
   "source": [
    "### Prepare and Setup Data\n",
    "Construct and/or load the datasets we need for the various experiments:\n",
    "- Linear Spectrogram <b>(r128xc128)</b>, latent dim: 256\n",
    "- Linear Spectrogram <b>(r512xc128)</b>, latent dim: 256\n",
    "- Linear Spectrogram <b>(r512xc128)</b>, latent dim: 256x4 (2048)\n",
    "\n",
    "Where rNUM indicated the FFT bins (i.e. the FFT-size) used to compute the spectrograms we are attempting to predict. cNUM is simply the STFT columns, which are a direct result of the <i>hop_size</i> we use.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8edb6",
   "metadata": {},
   "source": [
    "#### Globals for all experiments: \n",
    "Some variables are held constant throughout the different models under experimentation.\n",
    "We use the block below to define these, such that we can re-use them throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18215108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience lambda(s) to retrieve information\n",
    "# about a relevant .tfrecords file (nexamples, filepath)\n",
    "TFR_TRAIN      = lambda tfr_dict: tfr_dict[\"train\"]\n",
    "TFR_VALIDATION = lambda tfr_dict: tfr_dict[\"validation\"]\n",
    "TFR_TEST       = lambda tfr_dict: tfr_dict[\"test\"]\n",
    "TFR_PATH       = lambda tfr_tuple: tfr_tuple[-1]\n",
    "\n",
    "# Define a small lambda to compute the columns\n",
    "# to use for each nn input. TODO: FINISH\n",
    "NCOLS = lambda hl: FNCOLS(hl)\n",
    "\n",
    "# Define a small lambda to compute the rows\n",
    "# to use for each nn input.\n",
    "NROWS = lambda nfft: nfft//2 + 1\n",
    "\n",
    "# Define the train-size percentage.\n",
    "# This value indicates how much of the dataset is reserved for training. \n",
    "# The remaining is split evenly into validation/testing.\n",
    "# (i.e. a split could be 70-15-15)\n",
    "TRAIN_SIZE = 0.7\n",
    "\n",
    "# Define a flag to indicate whether we should\n",
    "# recompute all the datasets.\n",
    "DS_OVERWRITE = False\n",
    "\n",
    "# Define the beta-value to use for tuning the VAE(s).\n",
    "VAE_BETA = 1.\n",
    "\n",
    "# Define the learning rate to apply across\n",
    "# all training conditions.\n",
    "BASE_LR = 4e-04 \n",
    "\n",
    "# Define the optimizer to use for training across\n",
    "# all conditions.\n",
    "OPTIMIZER = optimizers.Adam(lr=BASE_LR)\n",
    "\n",
    "# Define batch size to use for training.\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Define a lambda to retrieve the step size(s).\n",
    "STEPS_PER_EPOCH = lambda tfr_tuple: tfr_tuple[0] // BATCH_SIZE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81afca",
   "metadata": {},
   "source": [
    "#### Approach 1 (AP1): Linspect r128xc128, latent dim 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfe3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FFT-size.\n",
    "ap1_nfft = 256\n",
    "\n",
    "# Define the hop_length (overlap)\n",
    "# in samples.\n",
    "ap1_overlap = ap1_nfft // 2\n",
    "\n",
    "# Define the latent dimension size.\n",
    "ap1_latent_dim = 256\n",
    "\n",
    "# Fetch a dataset that fullfils the \n",
    "# requirements for this condition.\n",
    "ap1_fout = get_dataset_for(\n",
    "    nfft               = ap1_nfft,\n",
    "    overlap            = ap1_overlap,\n",
    "    train_size         = TRAIN_SIZE, \n",
    "    overwrite_existing = DS_OVERWRITE\n",
    ")\n",
    "\n",
    "# Prepare the input/output shape(s).\n",
    "ap1_inout_shape_train = (NROWS(ap1_nfft), )\n",
    "ap1_inout_shape_eval  = (NROWS(ap1_nfft), NCOLS(ap1_overlap))  \n",
    "\n",
    "# Fetch a deep learning model (VAE) \n",
    "# for this condition.\n",
    "with strategy.scope():\n",
    "    ap1_vae = GaussianBetaVAE(\n",
    "        N                   = TFR_TRAIN(ap1_fout)[0],\n",
    "        M                   = BATCH_SIZE,\n",
    "        beta                = VAE_BETA,\n",
    "        input_dim           = ap1_inout_shape_train,\n",
    "        latent_dim          = ap1_latent_dim,\n",
    "        create_encoder_func = make_cnn_vae_encoder,\n",
    "        create_decoder_func = make_dense_vae_decoder\n",
    "    )\n",
    "\n",
    "    # Compile the model and make ready for training.\n",
    "    ap1_vae.custom_compile(optimizer=OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset(s) for training.\n",
    "ap1_ds_train = prepare_dataset_for_training(\n",
    "    filename     = TFR_PATH(TFR_TRAIN(ap1_fout)),\n",
    "    batch_size   = BATCH_SIZE,\n",
    "    cast_to_type = tf.float64\n",
    ")\n",
    "\n",
    "ap1_ds_validation = prepare_dataset_for_training(\n",
    "    filename     = TFR_PATH(TFR_VALIDATION(ap1_fout)),\n",
    "    batch_size   = BATCH_SIZE,\n",
    "    cast_to_type = tf.float64\n",
    ")\n",
    "\n",
    "# Train the vae model.\n",
    "ap1_history = ap1_vae.fit(\n",
    "    ap1_ds_train,\n",
    "    steps_per_epoch  = STEPS_PER_EPOCH(TFR_TRAIN(ap1_fout)),\n",
    "    validation_data  = ap1_ds_validation,\n",
    "    validation_steps = STEPS_PER_EPOCH(TFR_VALIDATION(ap1_fout)),\n",
    "    epochs           = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7267624b",
   "metadata": {},
   "source": [
    "#### Evaluate AP1 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc2374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "from data_utils.mg_sg_generator import get_dataset_small\n",
    "\n",
    "def sample_mvn(x_mu, x_logvar):\n",
    "    x_sig = np.exp(0.5*x_logvar)\n",
    "    batch = x_sig.shape[0]\n",
    "    mvn = tfp.distributions.MultivariateNormalDiag(\n",
    "        loc=x_mu, scale_diag=x_sig\n",
    "    )\n",
    "    return mvn.sample(shape=[batch]).numpy()\n",
    "\n",
    "def strip_file(filename):\n",
    "    return os.path.splitext(filename[filename.rindex('/') + 1:])[0]\n",
    "\n",
    "def prepare_dataset_for_evaluation(filename, cast_to_type=None):\n",
    "    # Fetch the necessary batching we have to do\n",
    "    # with the current memory-reduction setup\n",
    "    # TODO: explain memory reduction setup\n",
    "    batch_size = int(strip_file(filename).split('_')[3].split('x')[-1])\n",
    "    ds = get_dataset_small(filename, dtype=cast_to_type)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ap1_ds_test = prepare_dataset_for_evaluation(\n",
    "    filename     = TFR_PATH(TFR_TRAIN(ap1_fout)),\n",
    "    cast_to_type = tf.float64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7fa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_iter = iter(ap1_ds_test)\n",
    "mg_, sg_ = ds_iter.get_next()\n",
    "#mu_x, logvar_x, _, _ = ap1_vae.predict(mg_)\n",
    "#print(mu_x.shape, logvar_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "specgram_inv = sg_.numpy()#sample_mvn(mu_x, logvar_x)\n",
    "print(specgram_inv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529de5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ap1_iter = iter(ap1_ds_test)\n",
    "#specgram_inv = ap1_iter.get_next()[1].numpy()\n",
    "#print(specgram_inv.shape)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "axs.set_xticks([])\n",
    "axs.set_yticks([])\n",
    "axs.imshow(np.flipud(specgram_inv.T), aspect=\"auto\", cmap=\"Spectral_r\", interpolation=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = spectrogram2audio(specgram_inv.T, ap1_overlap, True)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a776814",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.display(ipd.Audio(y, rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5009918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
