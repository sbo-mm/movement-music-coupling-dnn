{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1153ec88",
   "metadata": {},
   "source": [
    "# 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b530c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture --no-display\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "currentdir = os.path.abspath(\".\")\n",
    "parentdir  = os.path.abspath(os.path.join(\".\", os.pardir))\n",
    "sys.path.insert(0, parentdir) \n",
    "\n",
    "RAND_STATE_GLOB = 1291\n",
    "REDO_FEATURE_EXTRACTION, EXTEND = False, False\n",
    "LOAD_PKL_DATA = True\n",
    "\n",
    "import time\n",
    "import joblib\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import savetxt\n",
    "import librosa as librosa\n",
    "import librosa.display as display\n",
    "import IPython.display as ipd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Deep Learning (Keras Setups)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils  import Sequence\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Flatten, Reshape, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, MaxPooling2D, Dropout, SpatialDropout2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.models import Model, load_model, model_from_json\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow_addons.layers import WeightNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2658d",
   "metadata": {},
   "source": [
    "### ii. Load  features from database files (*.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = currentdir + '/tmpdata/'\n",
    "audio_df = joblib.load(db_dir + \"audio_df.pkl\")\n",
    "video_df = joblib.load(db_dir + \"video_df.pkl\")\n",
    "inputs  = np.load(db_dir + 'inputs.npy')\n",
    "targets = np.load(db_dir + 'targets.npy')\n",
    "#print(inputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4f3aa7",
   "metadata": {},
   "source": [
    "#### iii. Setup global variables for spectrogram generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc3a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling rate\n",
    "sr = 22050\n",
    "\n",
    "# min/max freq \n",
    "fmin, fmax = 20, sr / 2 \n",
    "\n",
    "# number of samples for each fft window. \n",
    "# for music it is recommended 2048, but with 4096 we are getting better results\n",
    "n_fft = 4096\n",
    "\n",
    "#(columns) - so we can get 128 frames \n",
    "hop_length = 690\n",
    "\n",
    "#(rows) - With this, we get nice 128 x 128 spectrograms \n",
    "n_mels = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9a4ce",
   "metadata": {},
   "source": [
    "# 2. Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d62e2",
   "metadata": {},
   "source": [
    "#### 2.1 Setup a class that delivers a ML Model (CVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_PI = tf.constant(np.pi, dtype=K.floatx())\n",
    "\n",
    "def gaussian_log_prob(z, mu, logvar):\n",
    "    return -0.5*(tf.math.log(2.0*TF_PI) + logvar + tf.math.pow((z-mu), 2.0)/tf.math.exp(logvar))\n",
    "\n",
    "def log_mean_exp(x, axis):\n",
    "    m  = tf.math.reduce_max(x, axis=axis)\n",
    "    m2 = tf.math.reduce_max(x, axis=axis, keepdims=True)\n",
    "    return m + K.log(K.mean(K.exp(x-m2), axis))\n",
    "\n",
    "class ConvolutionalVariationalAutoEncoder(Model):\n",
    "    \n",
    "    def __init__(self, input_dims, latent_dim, lr, b=1.0, *args, **kwargs):\n",
    "        super(ConvolutionalVariationalAutoEncoder, self).__init__(*args, **kwargs)\n",
    "        self.input_dims = input_dims\n",
    "        self.input_dim  = self.input_dims[0] * self.input_dims[1]\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lr         = lr        \n",
    "        self.kernel     = (3,3)  \n",
    "        #self.act_layer  = LeakyReLU()\n",
    "        self.custom_optimizer = optimizers.Adam(lr=self.lr)\n",
    "        #self.loss       = 'mse'\n",
    "        #self.act_last_layer = 'sigmoid'\n",
    "        self.r = tf.constant(b, dtype=K.floatx())\n",
    "        \n",
    "        self._make_encoder()\n",
    "        self._make_decoder()\n",
    "    \n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "    \n",
    "    def get_decoder(self):\n",
    "        return self.decoder\n",
    "    \n",
    "    def _make_encoder(self):\n",
    "        '''Creates a field in the class representing the encoder model'''\n",
    "        \n",
    "        # Setup the input layer\n",
    "        input_dim_sca = self.input_dims[0] * self.input_dims[1]\n",
    "        self.input_encoder = Input(shape=(input_dim_sca,), name='input_encoder')\n",
    "        \n",
    "        # Setup hidden layers (Convolutional Layers)\n",
    "        encoder     = Reshape((self.input_dims[0], self.input_dims[1], 1))(self.input_encoder)\n",
    "        \n",
    "        encoder     = Conv2D(4, self.kernel, padding='same')(encoder)\n",
    "        encoder     = LeakyReLU()(encoder)\n",
    "        encoder     = MaxPooling2D((2, 2), padding='same')(encoder)\n",
    "        \n",
    "        encoder     = Conv2D(8, self.kernel, padding='same')(encoder)\n",
    "        encoder     = LeakyReLU()(encoder)\n",
    "        encoder     = MaxPooling2D((2, 2), padding='same')(encoder)\n",
    "        \n",
    "        encoder     = Conv2D(16, self.kernel, padding='same')(encoder)\n",
    "        encoder     = LeakyReLU()(encoder)\n",
    "        encoder     = MaxPooling2D((4, 4), padding='same')(encoder)\n",
    "        \n",
    "        encoder     = Flatten()(encoder)\n",
    "        \n",
    "        encoder     = Dense(2*latent_dim, name=\"encoder\",\n",
    "                            kernel_initializer=\"zeros\", \n",
    "                            bias_initializer='zeros')(encoder)\n",
    "        \n",
    "        self.encoder = Model(inputs=self.input_encoder, outputs=encoder)\n",
    "    \n",
    "    def _make_decoder(self):\n",
    "        # Setup the \"input\" layer for the decoder\n",
    "        input_dim_sca = self.input_dims[0] * self.input_dims[1]\n",
    "        input_latent = Input(shape=(self.latent_dim,), name='z_sampling')\n",
    "        \n",
    "        # Setup hidden layers (fully connected dense layers)        \n",
    "        decoder = Dense(input_dim_sca//4)(input_latent)\n",
    "        decoder = LeakyReLU()(decoder)\n",
    "        \n",
    "        decoder = Dense(2*input_dim_sca, activation=\"linear\")(decoder)\n",
    "        decoder = LeakyReLU()(decoder)\n",
    "        \n",
    "        # Full decoder model\n",
    "        self.decoder = Model(input_latent, decoder, name='decoder_vae')\n",
    "    \n",
    "    def custom_compile(self):\n",
    "        # Define some metric data displays\n",
    "        self.loss_ph  = tf.keras.metrics.Mean(name=\"ELBO\")\n",
    "        self.reg_loss = tf.keras.metrics.Mean(name=\"reg\")\n",
    "        self.rec_loss = tf.keras.metrics.Mean(name=\"rec\")\n",
    "        super(ConvolutionalVariationalAutoEncoder, self).compile()\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        _metrics = [\n",
    "            self.loss_ph,\n",
    "            self.reg_loss,\n",
    "            self.rec_loss\n",
    "        ]\n",
    "        return _metrics\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def encode(self, x, training=True):\n",
    "        z_params = self.encoder(x, training=training)\n",
    "        z_mu, z_logvar = tf.split(z_params, 2, 1)\n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    @tf.autograph.experimental.do_not_convert\n",
    "    def decode(self, z, training=True):\n",
    "        x_params = self.decoder(z, training=training)\n",
    "        x_mu, x_logvar = tf.split(x_params, 2, 1)\n",
    "        return x_mu, x_logvar\n",
    "    \n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = K.exp(0.5 * logvar)\n",
    "        return mu + tf.random.normal(tf.shape(std), dtype=K.floatx()) * std\n",
    "    \n",
    "    def call(self, x, training=True):\n",
    "        z_mu, z_logvar = self.encode(x, training=training)\n",
    "        z = self.reparametrize(z_mu, z_logvar)\n",
    "        x_mu, x_logvar = self.decode(z, training=training)\n",
    "        return x_mu, x_logvar, z_mu, z_logvar\n",
    "    \n",
    "    def compute_kld(self, z_mu, z_logvar):\n",
    "        return 0.5*(K.pow(z_mu, 2.0) + K.exp(z_logvar) - 1.0 - z_logvar)\n",
    "    \n",
    "    def compute_recon_loss(self, x, x_mu, x_logvar, log_prob_func):\n",
    "        return -K.sum(log_prob_func(x, x_mu, x_logvar), 1)\n",
    "    \n",
    "    def compute_negative_elbo(self, x, y, freebits=0.0, training=True):\n",
    "        x_mu_0, x_logvar_0, z_mu, z_logvar = self(x, training=training)\n",
    "        l_rec = self.compute_recon_loss(y, x_mu_0, x_logvar_0, gaussian_log_prob)\n",
    "        log2 = tf.cast(K.log(2.0), dtype=K.floatx())\n",
    "        freebits = tf.cast(freebits, dtype=K.floatx())\n",
    "        l_reg = K.sum(K.relu(self.compute_kld(z_mu, z_logvar) - freebits*log2) + freebits*log2, 1)\n",
    "        return l_rec + l_reg, l_rec, l_reg\n",
    "    \n",
    "    def importance_sampling(self, x, y, importance_samples=1, training=True):\n",
    "        z_mu, z_logvar = self.encode(x, training=training)\n",
    "\n",
    "        z_mu = tf.tile(tf.expand_dims(z_mu, 1), [1, importance_samples, 1])\n",
    "        z_mu = tf.reshape(z_mu, (-1, self.latent_dim))\n",
    "        z_logvar = tf.tile(tf.expand_dims(z_logvar, 1), [1, importance_samples, 1])\n",
    "        z_logvar = tf.reshape(z_logvar, (-1, self.latent_dim))        \n",
    "        x_0 = tf.tile(tf.expand_dims(y, 1), [1, importance_samples, 1])\n",
    "        x_0 = tf.reshape(x_0, (-1, self.input_dim))        \n",
    "\n",
    "        z = self.reparametrize(z_mu, z_logvar)\n",
    "\n",
    "        x_mu_0, x_logvar_0 = self.decode(z, training=training)\n",
    "\n",
    "        x_mu_0 = tf.reshape(x_mu_0, (-1, importance_samples, self.input_dim))\n",
    "        x_logvar_0 = tf.reshape(x_logvar_0, (-1, importance_samples, self.input_dim))\n",
    "   \n",
    "        x_0 = tf.reshape(x_0, (-1, importance_samples, self.input_dim))\n",
    "\n",
    "        z = tf.reshape(z, (-1, importance_samples, self.latent_dim))\n",
    "        z_mu = tf.reshape(z_mu, (-1, importance_samples, self.latent_dim))\n",
    "        z_logvar = tf.reshape(z_logvar, (-1, importance_samples, self.latent_dim))\n",
    "\n",
    "        logpxz_0 = K.sum(gaussian_log_prob(x_0, x_mu_0, x_logvar_0), -1)\n",
    "        \n",
    "        zeros_ = tf.zeros_like(z)\n",
    "        ones_  = tf.ones_like(z)\n",
    "        logpz  = K.sum(gaussian_log_prob(z, zeros_, zeros_), -1)\n",
    "        logqzx = K.sum(gaussian_log_prob(z, z_mu, z_logvar), -1)\n",
    "\n",
    "        logprob = logpxz_0+logpz - logqzx\n",
    "        logprob = log_mean_exp(logprob, 1)\n",
    "\n",
    "        return -logprob\n",
    "        \n",
    "    \n",
    "    def train_step(self, inputs):\n",
    "        x, y = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            log2 = tf.cast(K.log(2.0), dtype=K.floatx())        \n",
    "            neg_elbo, l_rec, l_reg = self.compute_negative_elbo(\n",
    "                x, y, freebits=0.05, training=True)           \n",
    "            loss = K.mean(self.r*l_reg + l_rec) / log2\n",
    "            self.loss_ph.update_state(loss)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.custom_optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.reg_loss.update_state(K.mean(self.r*l_reg)/log2)\n",
    "        self.rec_loss.update_state(K.mean(l_rec)/log2)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "        \n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        \n",
    "        # Compute predictions and update metrics\n",
    "        log2 = tf.cast(K.log(2.0), dtype=K.floatx())\n",
    "        nll = self.importance_sampling(x, y, 10, training=False)\n",
    "        loss = K.mean(nll) / log2\n",
    "        self.loss_ph.update_state(loss)\n",
    "\n",
    "        neg_elbo, l_rec, l_reg = self.compute_negative_elbo(\n",
    "            x, y, freebits=0.05, training=False)\n",
    "        self.reg_loss.update_state(K.mean(self.r*l_reg)/log2)\n",
    "        self.rec_loss.update_state(K.mean(l_rec)/log2)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb7cad",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fe467",
   "metadata": {},
   "source": [
    "#### 3.1 Setup global variables for NN (hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5ccc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims    = (128, 128)\n",
    "latent_dim    = 256\n",
    "batch_size    = 256\n",
    "epochs        = 300\n",
    "beta          = 150\n",
    "learning_rate = 4e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a49a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_model = ConvolutionalVariationalAutoEncoder(\n",
    "    input_dims, latent_dim, learning_rate)\n",
    "\n",
    "mg_model.custom_compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58765d6b",
   "metadata": {},
   "source": [
    "#### 3.2 Setup KFolds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa166fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create an instance of \"StratifiedKFold\" class\n",
    "# This class will perform the splits for us\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=4, shuffle=True, random_state=RAND_STATE_GLOB\n",
    ")\n",
    "\n",
    "# Create a list of integer labels on which the SKF can\n",
    "# perform its splits\n",
    "labels = list(video_df.loc[\"MusicEncoding\", :]) # <--- legacy, but keeping for now\n",
    "\n",
    "# Create a dummy array to represent features.\n",
    "# We are only interested in the indexes provided\n",
    "dummy_features = np.zeros((len(video_df.columns), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b872b",
   "metadata": {},
   "source": [
    "### 3.3 Begin  Training Using KFolds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373d69f",
   "metadata": {},
   "source": [
    "##### 3.3.3 Setup some variables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adef002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to the directory where we store the weights\n",
    "savedir = currentdir + '/saved_models_03/'\n",
    "\n",
    "# A small lambda to set a model save name\n",
    "get_model_name = lambda k: 'new_model_'+str(k)+'.h5'\n",
    "\n",
    "# A constant to determine if we should load our model before training\n",
    "LOAD_BEFORE_TRAINING = False\n",
    "IGNORE_TRAINED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8364194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An indexer to label the current fold\n",
    "fold_var = 1\n",
    "\n",
    "# A list of allready \"trained models\"\n",
    "models_list = os.listdir(savedir)\n",
    "\n",
    "# Perform the K splits \n",
    "for train, test in skf.split(dummy_features, labels):\n",
    "        \n",
    "    model_name = get_model_name(fold_var)\n",
    "    if (model_name in models_list) and not IGNORE_TRAINED:\n",
    "        fold_var += 1\n",
    "        print(\"skipping training for \" + model_name)\n",
    "        continue\n",
    "    \n",
    "    NO_LOAD = True\n",
    "    \n",
    "    # Print the samples in the test-train split\n",
    "    print(train.shape, test.shape)\n",
    "    \n",
    "    # Create our partitions\n",
    "    trainX, trainY = inputs[train, :], targets[train, :]\n",
    "    testX , testY  = inputs[test,  :], targets[test,  :]\n",
    "    \n",
    "    # Get an instance of the CVAE model\n",
    "    model  = ConvolutionalVariationalAutoEncoder(\n",
    "        input_dims, latent_dim, learning_rate, b=beta)\n",
    "    model.custom_compile()\n",
    "    #model = cvae.get_full_model()\n",
    "    \n",
    "    if LOAD_BEFORE_TRAINING and not NO_LOAD:\n",
    "        model.load_weights(savedir + model_name)\n",
    "\n",
    "    # Setup saving weights incrementally\n",
    "    sf = int((inputs.shape[0] // batch_size + 1) * 15)\n",
    "    checkpoint = ModelCheckpoint(savedir + model_name, monitor='ELBO', verbose=1, \n",
    "          save_best_only=True, mode='auto', save_freq=sf, save_weights_only=True)\n",
    "    callbacks  = [checkpoint]\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        trainX, trainY,\n",
    "        epochs          = 5,\n",
    "        batch_size      = batch_size,\n",
    "        validation_data = (testX, testY),\n",
    "        shuffle         = True,\n",
    "        verbose         = 1\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        trainX, trainY,\n",
    "        epochs          = epochs - 5,\n",
    "        batch_size      = batch_size,\n",
    "        validation_data = (testX, testY),\n",
    "        shuffle         = True,\n",
    "        verbose         = 1,\n",
    "        callbacks       = callbacks\n",
    "    )\n",
    "    \n",
    "    print('Done in {0:.2f} seconds'.format((time.time() - start_time)))\n",
    "    \n",
    "    # Save history with joblib\n",
    "    joblib.dump(\n",
    "        history.history,\n",
    "        savedir + 'new_model_{0}_history.pkl'.format(fold_var), \n",
    "        compress = 3\n",
    "    )\n",
    "            \n",
    "    K.clear_session()\n",
    "    fold_var += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255b25d",
   "metadata": {},
   "source": [
    "#### 3.4 Train the original model with the full data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d50a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = \"model_mg.h5\"\n",
    "\n",
    "# Get an instance of the CVAE model\n",
    "model  = ConvolutionalVariationalAutoEncoder(\n",
    "    input_dims, latent_dim, learning_rate, b=beta)\n",
    "model.custom_compile()\n",
    "#model = cvae.get_full_model()\n",
    "\n",
    "# Setup model saving\n",
    "sf = int((inputs.shape[0] // batch_size + 1) * 15)\n",
    "checkpoint = ModelCheckpoint(savedir + model_save_name, monitor='ELBO', verbose=1, \n",
    "      save_best_only=True, mode='auto', save_freq=sf, save_weights_only=True)\n",
    "callbacks  = [checkpoint]\n",
    "\n",
    "#inputs_tf = tf.convert_to_tensor(inputs)\n",
    "#targets_tf = tf.convert_to_tensor(targets)\n",
    "\n",
    "history_mg = model.fit(\n",
    "    inputs, targets,\n",
    "    epochs          = 300,\n",
    "    batch_size      = batch_size,\n",
    "    callbacks       = callbacks,\n",
    "    shuffle         = True,\n",
    "    verbose         = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba243058",
   "metadata": {},
   "source": [
    "# (4). Pre-Evaluation By Metrics Inspections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04818d",
   "metadata": {},
   "source": [
    "#### (4).1 Evaluate the K-Folds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store results of the different KFolds\n",
    "TRAIN_R2 = []\n",
    "TRAIN_LOSS = []\n",
    "VALIDATION_R2 = []\n",
    "VALIDATION_LOSS = []\n",
    "\n",
    "fold_var = 1\n",
    "for train, test in skf.split(dummy_features, labels):\n",
    "    model_name = get_model_name(fold_var)\n",
    "    \n",
    "    # Create our partitions\n",
    "    trainX, trainY = inputs[train, :], targets[train, :]\n",
    "    testX , testY  = inputs[test,  :], targets[test,  :]\n",
    "    \n",
    "    # Get an instance of the CVAE model\n",
    "    model  = ConvolutionalVariationalAutoEncoder(\n",
    "        input_dims, latent_dim, learning_rate)\n",
    "    model.custom_compile()\n",
    "    model(trainX[:1])\n",
    "    model.load_weights(savedir + model_name)\n",
    "    \n",
    "    #Fetch the evaluation metrics\n",
    "    results_train = model.evaluate(trainX, trainY)\n",
    "    results_train = dict(zip(model.metrics_names, results_train))\n",
    "    results_test  = model.evaluate(testX, testY)\n",
    "    results_test  = dict(zip(model.metrics_names, results_test))\n",
    "    \n",
    "    # Save metrics in a list\n",
    "    TRAIN_R2.append(results_train[\"rec\"])\n",
    "    TRAIN_LOSS.append(results_train[\"ELBO\"])\n",
    "    VALIDATION_R2.append(results_test[\"rec\"])\n",
    "    VALIDATION_LOSS.append(results_test[\"ELBO\"])\n",
    "    \n",
    "    K.clear_session()\n",
    "    fold_var += 1\n",
    "\n",
    "print(TRAIN_R2)\n",
    "print(VALIDATION_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af47f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean and std of the kfolds metrics\n",
    "mu_train = np.mean(TRAIN_R2)\n",
    "sig_train = np.std(TRAIN_R2)\n",
    "print(mu_train, sig_train)\n",
    "\n",
    "mu_test = np.mean(VALIDATION_R2)\n",
    "sig_test = np.std(VALIDATION_R2)\n",
    "print(mu_test, sig_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "data = {\n",
    "    \"ELBO\": [],\n",
    "    \"val_ELBO\": [],\n",
    "    \"rec\": [],\n",
    "    \"val_rec\": []\n",
    "}\n",
    "\n",
    "for i in range(4):\n",
    "    fold_var = i + 1\n",
    "    history_path = savedir + \"model_\" + str(fold_var) + \"_history.pkl\"\n",
    "    history = joblib.load(history_path)\n",
    "    data[\"ELBO\"].append(history[\"ELBO\"])\n",
    "    data[\"val_ELBO\"].append(history[\"val_ELBO\"])\n",
    "    data[\"rec\"].append(history[\"rec\"])\n",
    "    data[\"val_rec\"].append(history[\"val_rec\"])\n",
    "\n",
    "elbo_np = np.array(data[\"ELBO\"])\n",
    "elbo_np_avg = np.mean(elbo_np, axis=0)\n",
    "val_elbo_np = np.array(data[\"val_ELBO\"])\n",
    "val_elbo_avg = np.mean(val_elbo_np, axis=0)\n",
    "\n",
    "rec_np = np.array(data[\"rec\"])\n",
    "rec_avg = np.mean(rec_np, axis=0)\n",
    "val_rec_np = np.array(data[\"val_rec\"])\n",
    "val_rec_avg = np.mean(val_rec_np, axis=0)\n",
    "\n",
    "# Plot loss history across epochs\n",
    "fig, axs = plt.subplots(2, 1, figsize=(5, 7), dpi=120)\n",
    "axs[0].plot([], [], label=\"Train Loss\", color=\"green\")\n",
    "axs[0].plot([], [], label=\"Validation Loss\", color=\"orange\")\n",
    "axs[1].plot([], [], label=\"Train Loss\", color=\"green\")\n",
    "axs[1].plot([], [], label=\"Validation Loss\", color=\"orange\")\n",
    "\n",
    "for i in range(4):    \n",
    "    axs[0].plot(elbo_np[i, :], color=\"green\", alpha=0.2, linewidth=0.5)\n",
    "    axs[0].plot(val_elbo_np[i, :], color=\"orange\", alpha=0.2, linewidth=0.5)\n",
    "    axs[1].plot(rec_np[i, :], color=\"green\", alpha=0.2, linewidth=0.5)\n",
    "    axs[1].plot(val_rec_np[i, :], color=\"orange\", alpha=0.2, linewidth=0.5)\n",
    "    \n",
    "axs[0].plot(elbo_np_avg, color=\"darkgreen\", linewidth=0.9)\n",
    "axs[0].plot(val_elbo_avg, color=\"orange\", linewidth=0.9)\n",
    "axs[1].plot(rec_avg, color=\"darkgreen\", linewidth=0.9)\n",
    "axs[1].plot(val_rec_avg, color=\"orange\", linewidth=0.9)\n",
    "    \n",
    "#axs[0].set_xlabel('Epochs')\n",
    "axs[0].set_xticklabels([])\n",
    "axs[0].set_ylabel('Loss (Negative ELBO)')\n",
    "axs[1].set_xlabel('Epochs')\n",
    "axs[1].set_ylabel('Reconstruction Loss')\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff3321",
   "metadata": {},
   "source": [
    "# 4. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b313d75c",
   "metadata": {},
   "source": [
    "#### 4.1 Visualise Input-Output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4872e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load an instance of the trained model\n",
    "model_name = \"new_model_1.h5\"\n",
    "\n",
    "mg_model = ConvolutionalVariationalAutoEncoder(\n",
    "    input_dims, latent_dim, learning_rate)\n",
    "\n",
    "mg_model.custom_compile()\n",
    "mg_model(inputs[:1])\n",
    "mg_model.load_weights(\"./saved_models_03/\" + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628cf45",
   "metadata": {},
   "source": [
    "##### (4.1) Fetch new unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c550770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "def sample_mvn(x_mu, x_logvar):\n",
    "    x_std = np.exp(x_logvar)\n",
    "    batch = x_std.shape[0]\n",
    "    \n",
    "    mvn = tfp.distributions.MultivariateNormalDiag(\n",
    "        loc=x_mu, scale_diag=x_std\n",
    "    )\n",
    "    return mvn.sample(shape=[batch]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775aea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgs_test = joblib.load(currentdir + \"/tmpdata/test_mgs.pkl\")\n",
    "print(len(mgs_test))\n",
    "\n",
    "test_idx = 2\n",
    "mg_to_test = mgs_test[test_idx][1]\n",
    "mg_to_test = cv2.cvtColor(mg_to_test, cv2.COLOR_RGB2GRAY)\n",
    "mg_to_test = cv2.resize(mg_to_test, (128, 128))\n",
    "mg_to_test = mg_to_test.astype(np.float32)\n",
    "mg_to_test = np.clip(mg_to_test / 255.0, 0, 1)\n",
    "mg_to_test = mg_to_test.flatten()\n",
    "mg_to_test = np.expand_dims(mg_to_test, axis=0)\n",
    "\n",
    "\n",
    "# Make a prediction\n",
    "eval_pred_target = mg_model.predict(mg_to_test)\n",
    "\n",
    "# Transform back to an \"image\" representation\n",
    "mel_pred = np.reshape(eval_pred_target, input_dims)\n",
    "mel_pred = (1 - mel_pred) * -80.\n",
    "mg       = np.reshape(mg_to_test, input_dims)\n",
    "\n",
    "\n",
    "# Plot the two mel-spectrograms\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "axs[0].set_xlabel(\"(a) Input Motiongram\")\n",
    "axs[0].imshow(mg, aspect=\"auto\", cmap=\"binary\", interpolation=\"bicubic\")\n",
    "\n",
    "librosa.display.specshow(mel_pred, x_axis='time', y_axis='mel', \n",
    "                         cmap='binary', fmin=fmin, fmax=fmax, ax=axs[1]);\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_ylabel('')\n",
    "axs[1].set_xlabel(\"(b) Predicted Spectrogram\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "mel_pred_pwr = librosa.db_to_power(mel_pred)\n",
    "y_pred = librosa.feature.inverse.mel_to_audio(\n",
    "    mel_pred_pwr, sr=sr, n_fft=n_fft, hop_length=hop_length, \n",
    "    window=scipy.signal.hamming, fmin=fmin, fmax=fmax\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Audio(y_pred, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336590b2",
   "metadata": {},
   "source": [
    "##### 4.1.1 SANITY CHECK: We try to pass through known and unknown inputs to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample number\n",
    "example_number = 788\n",
    "\n",
    "# Extract two examples:\n",
    "# (1) a sample to feed through the network\n",
    "# (2) the ground truth to compare againts\n",
    "eval_input       = np.expand_dims( inputs[example_number, :], axis=0)\n",
    "eval_real_target = np.expand_dims(targets[example_number, :], axis=0)\n",
    "print(eval_input.shape, eval_real_target.shape)\n",
    "\n",
    "# Make a prediction\n",
    "x_mu, x_logvar, _, _ = mg_model.predict(eval_input)\n",
    "\n",
    "eval_pred_target = sample_mvn(x_mu, x_logvar)\n",
    "print(eval_pred_target.shape)\n",
    "\n",
    "# Transform back to an \"image\" representation\n",
    "mel_real = np.reshape(eval_real_target, input_dims)\n",
    "mel_real = (1 - mel_real) * -80.\n",
    "mel_pred = np.reshape(eval_pred_target, input_dims)\n",
    "mel_pred = (1 - mel_pred) * -80.\n",
    "mg       = np.reshape(eval_input, input_dims)\n",
    "\n",
    "# Plot the two mel-spectrograms\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), dpi=120)\n",
    "\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "axs[0].set_xlabel(\"(a) Input Motiongram\")\n",
    "axs[0].imshow(mg, aspect=\"auto\", cmap=\"binary\", interpolation=\"bicubic\")\n",
    "\n",
    "librosa.display.specshow(mel_real, x_axis='time', y_axis='mel', \n",
    "                         cmap='binary', fmin=fmin, fmax=fmax, ax=axs[1]);\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_ylabel('')\n",
    "axs[1].set_xlabel(\"(b) Associated Spectrogram\")\n",
    "\n",
    "librosa.display.specshow(mel_pred, x_axis='time', y_axis='mel', \n",
    "                         cmap='binary', fmin=fmin, fmax=fmax, ax=axs[2]);\n",
    "axs[2].set_xticks([])\n",
    "axs[2].set_yticks([])\n",
    "axs[2].set_ylabel('')\n",
    "axs[2].set_xlabel(\"(c) Predicted Spectrogram\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plot audio examples:\n",
    "mel_real_pwr = librosa.db_to_power(mel_real)\n",
    "y_real = librosa.feature.inverse.mel_to_audio(\n",
    "    mel_real_pwr, sr=sr, n_fft=n_fft, hop_length=hop_length, \n",
    "    window=scipy.signal.hamming, fmin=fmin, fmax=fmax\n",
    ")\n",
    "\n",
    "mel_pred_pwr = librosa.db_to_power(mel_pred)\n",
    "y_pred = librosa.feature.inverse.mel_to_audio(\n",
    "    mel_pred_pwr, sr=sr, n_fft=n_fft, hop_length=hop_length, \n",
    "    window=scipy.signal.hamming, fmin=fmin, fmax=fmax\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Audio(y_real, rate=sr))\n",
    "ipd.display(ipd.Audio(y_pred, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf55829",
   "metadata": {},
   "source": [
    "##### 4.2.1 Visualize the latent space conditioned on the audio-sample labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f604eec",
   "metadata": {},
   "source": [
    "#### 4.2 Experiment with the latent space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the encoder and decoder separately\n",
    "mg_encoder = mg_model.get_encoder()\n",
    "mg_decoder = mg_model.get_decoder()\n",
    "\n",
    "def encode(features):\n",
    "    z_mu, z_logvar = np.split(mg_encoder.predict(features), 2, axis=-1)\n",
    "    return mg_model.reparametrize(z_mu, z_logvar).numpy()\n",
    "\n",
    "def decode(z):\n",
    "    x_mu, x_logvar = np.split(mg_decoder.predict(z), 2, axis=-1)\n",
    "    return sample_mvn(x_mu, x_logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceddf438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the \"features\" (which was previously called inputs)\n",
    "features = inputs.copy()\n",
    "\n",
    "# Project features to latent space\n",
    "z_mu, z_logvar = np.split(mg_encoder.predict(features), 2, axis=-1)\n",
    "z = mg_model.reparametrize(z_mu, z_logvar).numpy()\n",
    "print(z.shape)\n",
    "\n",
    "corr = np.corrcoef(z[:, :16].T)\n",
    "print(corr.shape)\n",
    "\n",
    "sns.heatmap(corr, annot=False, xticklabels=True, yticklabels=True, cmap='Spectral_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the \"features\" (which was previously called inputs)\n",
    "features = inputs.copy()\n",
    "\n",
    "# Grab the label (genre encoding) for each feature\n",
    "labels_str  = list(video_df.loc[\"DanceGenre\", :])\n",
    "dancegenres = set(labels_str)\n",
    "genre_dict  = {k: idx for idx, k in enumerate(dancegenres)}\n",
    "label_genre = {v: k for k, v in genre_dict.items()}\n",
    "labels      = [genre_dict[k] for k in list(video_df.loc[\"DanceGenre\", :])]\n",
    "\n",
    "feature_labels = np.array(labels)\n",
    "print(features.shape, feature_labels.shape)\n",
    "\n",
    "# Perform LDA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "components = 3\n",
    "lda = LinearDiscriminantAnalysis(n_components=components)\n",
    "\n",
    "# Project features to latent space\n",
    "z_mu, z_logvar = np.split(mg_encoder.predict(features), 2, axis=-1)\n",
    "latent_features = mg_model.reparametrize(z_mu, z_logvar).numpy()\n",
    "print(latent_features.shape)\n",
    "\n",
    "# Split to train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        latent_features, feature_labels, test_size=0.20, random_state=19)\n",
    "\n",
    "# Fit the latent features\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "# Perform a transformation and plot\n",
    "X = lda.transform(X_train)\n",
    "print(X.shape)\n",
    "\n",
    "# Visualize on a scatter plot\n",
    "fig = plt.figure(figsize=(5, 8), dpi=120)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "#ax1 = fig.add_subplot(projection='3d')\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "colors = [plt.cm.Spectral_r(i/float(len(genre_dict)-1)) for i in range(len(genre_dict))]\n",
    "colors = ListedColormap(colors)\n",
    "\n",
    "scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_train, cmap=colors)\n",
    "ax.legend(\n",
    "    handles=scatter.legend_elements()[0], \n",
    "    labels=genre_dict.keys(),\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, 1.2),\n",
    "    ncol=3, fancybox=True, shadow=True\n",
    ")\n",
    "ax.set_xlabel(\"LDA 1\")\n",
    "ax.set_ylabel(\"LDA 2\")\n",
    "ax.set_zlabel(\"LDA 3\")\n",
    "#plt.figure(figsize=(7, 6))\n",
    "#plt.scatter(X[:, 0], X[:, 1], c=y_train, cmap='Spectral_r')\n",
    "#ax.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Predict train accuracy\n",
    "y_pred_train = lda.predict(X_train)\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "# Predict test accuracy\n",
    "y_pred_test = lda.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Print results\n",
    "print(acc_train, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_zs = np.random.normal(size=(3120, latent_dim), loc=0, scale=1)\n",
    "\n",
    "X = lda.transform(many_zs)\n",
    "y = lda.predict(many_zs)\n",
    "\n",
    "# Visualize on a scatter plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='Spectral_r')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58f0a0",
   "metadata": {},
   "source": [
    "##### 4.2.2 Combine motiongrams in the latent space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch some random examples\n",
    "num_examples = 3\n",
    "rand_idx = np.random.randint(inputs.shape[0], size=num_examples)\n",
    "examples = inputs[rand_idx, :]\n",
    "lat_sum  = np.zeros((1, latent_dim))\n",
    "lat_fac  = 1. / num_examples\n",
    "\n",
    "for ii in range(num_examples):\n",
    "    ex = np.expand_dims(examples[ii, :], axis=0)\n",
    "    _, _, zs = mg_encoder.predict(ex)\n",
    "    lat_sum = (lat_sum + zs) * lat_fac\n",
    "\n",
    "print(lat_sum.shape)    \n",
    "decoded_mel = mg_decoder.predict(lat_sum)\n",
    "decoded_mel = np.reshape(decoded_mel, input_dims)\n",
    "decoded_mel = (1 - decoded_mel) * -80.\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "librosa.display.specshow(decoded_mel, x_axis='time', y_axis='mel', \n",
    "                         cmap='binary', fmin=fmin, fmax=fmax, ax=axs);\n",
    "plt.show()\n",
    "\n",
    "decoded_mel = librosa.db_to_power(decoded_mel)\n",
    "y_decoded = librosa.feature.inverse.mel_to_audio(\n",
    "    decoded_mel, sr=sr, n_fft=n_fft, hop_length=hop_length, \n",
    "    window=scipy.signal.hamming, fmin=fmin, fmax=fmax\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Audio(y_decoded, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5ba3cc",
   "metadata": {},
   "source": [
    "##### 4.2.3 Sample randomly and generate an audio-hook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9acf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a \"latent\" space vector using numpy\n",
    "zs = np.random.normal(size=(1, latent_dim), loc=0.25, scale=3.95)\n",
    "\n",
    "# Use our lda to predict which (dance) genre this is\n",
    "y = lda.predict(zs)\n",
    "print(\"Genre: \" + label_genre[int(y)])\n",
    "\n",
    "# Pass sample through decoder\n",
    "decoded_mel = mg_decoder.predict(zs)\n",
    "decoded_mel = np.reshape(decoded_mel, input_dims)\n",
    "decoded_mel = (1 - decoded_mel) * -80.\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "librosa.display.specshow(decoded_mel, x_axis='time', y_axis='mel', \n",
    "                         cmap='binary', fmin=fmin, fmax=fmax, ax=axs);\n",
    "plt.show()\n",
    "\n",
    "decoded_mel = librosa.db_to_power(decoded_mel)\n",
    "y_decoded = librosa.feature.inverse.mel_to_audio(\n",
    "    decoded_mel, sr=sr, n_fft=n_fft, hop_length=hop_length, \n",
    "    window=scipy.signal.hamming, fmin=fmin, fmax=fmax\n",
    ")\n",
    "\n",
    "ipd.display(ipd.Audio(y_decoded, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edefdb85",
   "metadata": {},
   "source": [
    "##### 4.2.4 Interpolate between two motiongrams (in latent space) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random number\n",
    "rand_idxs = np.random.randint(inputs.shape[0], size=2)\n",
    "print(rand_idxs)\n",
    "\n",
    "# Fetch two examples to interpolate between\n",
    "inputs_subset = inputs[rand_idxs, :]\n",
    "example_lo    = np.expand_dims(inputs_subset[0, :], axis=0)\n",
    "example_hi    = np.expand_dims(inputs_subset[1, :], axis=0)\n",
    "\n",
    "# Project them both to latent space\n",
    "_, _, zs_lo = mg_encoder.predict(example_lo)\n",
    "_, _, zs_hi = mg_encoder.predict(example_hi)\n",
    "\n",
    "steps   = 5\n",
    "interps = []\n",
    "\n",
    "fig, axs = plt.subplots(1, steps, figsize=(15, 5))\n",
    "\n",
    "for ii in range(steps):\n",
    "    # Linearly interpolate the latent vector\n",
    "    tau = ii / (steps - 1)\n",
    "    zs  = zs_lo*(1 - tau) + zs_hi*tau\n",
    "    \n",
    "    # Try to predict which genre comes in between\n",
    "    y = lda.predict(zs)\n",
    "    print(\"Genre: \" + label_genre[int(y)])\n",
    "    \n",
    "    # Decode it back to a \"spectrogram\"\n",
    "    decoded_mel = mg_decoder.predict(zs)\n",
    "    decoded_mel = np.reshape(decoded_mel, input_dims)\n",
    "    decoded_mel = (1 - decoded_mel) * -80.\n",
    "    \n",
    "    # Display it\n",
    "    axs[ii].set_xticks([])\n",
    "    axs[ii].set_yticks([])\n",
    "    axs[ii].set_title(\"Step {}\".format(ii+1), size=12)\n",
    "    axs[ii].imshow(decoded_mel, interpolation = \"bicubic\", cmap = \"binary\")\n",
    "    \n",
    "    # Convert to audio\n",
    "    decoded_mel_pwr = librosa.db_to_power(decoded_mel)\n",
    "    y_decoded = librosa.feature.inverse.mel_to_audio(\n",
    "        decoded_mel_pwr, sr=sr, n_fft=n_fft, hop_length=hop_length, \n",
    "        window=scipy.signal.hamming, fmin=fmin, fmax=fmax\n",
    "    )\n",
    "    \n",
    "    # Store in list\n",
    "    interps.append((decoded_mel, y_decoded, zs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f84394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play some of the audio samples\n",
    "#step_idx = 2\n",
    "#y_       = interps[step_idx][1] \n",
    "for _, y_, _ in interps:\n",
    "    ipd.display(ipd.Audio(y_, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e543c0",
   "metadata": {},
   "source": [
    "#### 4.3 Visualize activations in first conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [\n",
    "    layer.output for layer in mg_encoder.layers[:3]\n",
    "] \n",
    "\n",
    "activation_model = Model(\n",
    "    inputs=mg_cvae.input_encoder, \n",
    "    outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d05db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_no = 310\n",
    "activations = activation_model.predict(\n",
    "    np.reshape(inputs[example_no], (1, -1))\n",
    ") \n",
    "\n",
    "fig = plt.figure(figsize=[14,14])\n",
    "for f in range(0,32):\n",
    "    plt.subplot(6, 6, f+1)\n",
    "    fig = plt.imshow(activations[2][0,:,:,f], cmap='binary', origin='lower')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd026da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2befe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
